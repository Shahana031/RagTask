{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07c66596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated 50 labelled chunks for embeddings.\n",
      " Loading existing FAISS index...\n",
      " FAISS index loaded successfully!\n",
      "\n",
      " Smart Query Assistant ready! Type 'exit' to quit.\n",
      "\n",
      "You: what are the names of different products?\n",
      " Detected Intent: numeric\n",
      "\n",
      " Generated SQL query:\n",
      "SELECT DISTINCT \"Product Name\" FROM orders;\n",
      " SQLGlot syntax check passed.\n",
      " Validating SQL query: SELECT DISTINCT \"Product Name\" FROM orders;\n",
      " SQL validation passed.\n",
      "Assistant: The SQL query returned a list of various products, primarily household items and appliances. Here are the names of the products included in the results:\n",
      "\n",
      "1. Air Conditioner\n",
      "2. Washing Machine\n",
      "3. Microwave\n",
      "4. Wall Tiles\n",
      "5. Heater\n",
      "6. Dining Table\n",
      "7. Bed\n",
      "8. Curtain\n",
      "9. Refrigerator\n",
      "10. Chair\n",
      "11. Sofa\n",
      "12. Floor Tiles\n",
      "13. Bathtub\n",
      "14. Porcelain Sink\n",
      "15. Wall Art\n",
      "16. Wardrobe\n",
      "17. Toilet Bowl\n",
      "18. Lamp\n",
      "19. Mirror\n",
      "\n",
      "This list encompasses a range of items that are commonly found in homes, including appliances for heating and cooling, furniture for living and dining spaces, and decorative elements. Each product serves a specific function, contributing to the comfort and aesthetics of a living environment.\n",
      "\n",
      "You: What types of products are popular for home decoration?\n",
      " Error: Connection error.\n",
      "Please try again with a different question.\n",
      "\n",
      "You: What types of products are popular for home decoration?\n",
      " Detected Intent: semantic\n",
      "Assistant: The dataset includes several types of products categorized under \"Decor,\" which are popular for home decoration. The specific products listed are:\n",
      "\n",
      "1. Curtain\n",
      "2. Mirror\n",
      "3. Wall Art\n",
      "4. Lamp\n",
      "\n",
      "Among these, curtains and mirrors appear multiple times, indicating they may be particularly popular choices for home decoration.\n",
      "\n",
      "You: exit\n",
      "Assistant: Goodbye !\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List ,  Optional\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.documents import Document \n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from sqlglot import parse_one\n",
    "import json\n",
    "\n",
    "#  Step 1: Define paths\n",
    "csv_path = r\"D:\\RAG Task\\Client_Shipment_Orders.csv\"\n",
    "db_path = r\"D:\\RAG Task\\orders.duckdb\"\n",
    "faiss_index_path = r\"D:\\RAG Task\\faiss_index\"\n",
    "\n",
    "#  Step 2: Create or update database table (runs only once)\n",
    "with duckdb.connect(db_path) as con:\n",
    "    # Create the 'orders' table if not already present\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS orders AS\n",
    "        SELECT * FROM read_csv_auto('{csv_path}');\n",
    "    \"\"\")\n",
    "    # Optional: Refresh data if you've updated CSV\n",
    "    # con.execute(f\"DELETE FROM orders; INSERT INTO orders SELECT * FROM read_csv_auto('{csv_path}');\")\n",
    "\n",
    "#  Step 3: Load DataFrame safely for local use\n",
    "with duckdb.connect(db_path) as con:\n",
    "    df = con.execute(\"SELECT * FROM orders\").fetchdf()\n",
    "\n",
    "#  Step 4: Clean / normalize text columns\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[col] = df[col].astype(str).str.strip().str.title()\n",
    "\n",
    "sql_system_prompt = \"\"\"\n",
    "You are a SQL expert helping to query a DuckDB table named `orders`.\n",
    "\n",
    "----------------------------------\n",
    "TABLE INFORMATION\n",
    "----------------------------------\n",
    "Table name: orders  \n",
    "Columns and their meanings:\n",
    "- Order ID: Unique identifier for each order (text)\n",
    "- Client Name: Name of the customer who placed the order (text)\n",
    "- Email: Email address of the client (text)\n",
    "- Contact Number: Client's contact phone number (text)\n",
    "- Origin: Source location of the shipment (text)\n",
    "- Destination: Delivery location of the shipment (text)\n",
    "- Product Name: Name of the purchased product (text)\n",
    "- Category: Product category (e.g., Furniture, Decor, Appliances)\n",
    "- Material: Material type of the product (e.g., Wood, Glass, Metal)\n",
    "- Color: Color of the product (text)\n",
    "- Quantity: Number of units ordered (integer)\n",
    "- Unit Price (‚Çπ): Price per unit in INR (numeric)\n",
    "- Total Price (‚Çπ): Total order price in INR (numeric)\n",
    "- Order Date: Date when the order was placed (date)\n",
    "- Delivery Date: Date when the order was delivered (date)\n",
    "- Status: Order status (e.g., Delivered, Pending, Cancelled)\n",
    "\n",
    "----------------------------------\n",
    "SAMPLE DATA\n",
    "----------------------------------\n",
    "ORD0001 | Kara Mata | chelsea75@yahoo.com | 038.830.3017x8206 | Port Mariamouth | Cohenmouth | Wall Art | Decor | Glass | Grey | 15 | 29878 | 448170 | 2025-05-13 | 2025-06-02 | Cancelled  \n",
    "ORD0002 | Jesse Williams | ccasey@barrett.info | (426)505-2355 | Tamaraview | Lake Rickyport | Bed | Furniture | Glass | Brown | 30 | 1507 | 45210 | 2025-10-04 | 2025-11-03 | Cancelled  \n",
    "\n",
    "----------------------------------\n",
    "INSTRUCTIONS\n",
    "----------------------------------\n",
    "1. Generate SQL queries **only** for structured or numeric filters.\n",
    "   Examples:\n",
    "   - Total sales, sum, count, average, quantity, or price-based questions  \n",
    "   - Filtering by columns such as Status, Category, Material, or Color  \n",
    "   - Date-based filters (e.g., orders after 2025-05-01)\n",
    "\n",
    "2. **Do NOT** generate queries based on subjective or descriptive logic\n",
    "   such as reasons for cancellation, customer feedback, or preferences.\n",
    "   These are handled separately by a semantic retriever system.\n",
    "\n",
    "3. Use the correct table name `orders` and column names **exactly as shown**.\n",
    "   Preserve proper case and special characters (e.g., `\"Total Price (‚Çπ)\"`).\n",
    "\n",
    "4. Never hallucinate columns, tables, or calculations that do not exist.\n",
    "\n",
    "5. Return **only** the SQL query ‚Äî no markdown, comments, or explanations.\n",
    "\n",
    "6. **SAFETY RULES ‚Äî STRICTLY ENFORCED**\n",
    "   - Never modify or delete data.\n",
    "   - Do not use or suggest `UPDATE`, `DELETE`, `INSERT`, `DROP`, `TRUNCATE`, or `ALTER`.\n",
    "   - Do not create or alter schemas, indexes, or tables.\n",
    "   - Only allow read-only operations:  \n",
    "     `SELECT`, `WHERE`, `GROUP BY`, `ORDER BY`, `LIMIT`, and aggregate functions (`COUNT`, `SUM`, `AVG`, `MIN`, `MAX`).\n",
    "\n",
    "7. **Case Handling:**  \n",
    "   When matching text values (like product or status), use `LOWER()` to make comparisons case-insensitive.  \n",
    "   Example:  \n",
    "   `WHERE LOWER(\"Product Name\") = LOWER('Toilet Bowl')`\n",
    "\n",
    "8. **Special Handling ‚Äî Highest or Maximum Queries:**  \n",
    "   If the user asks questions like  \n",
    "   *\"Who made the highest purchase?\"*,  \n",
    "   *\"Which client has the largest total?\"*, or  \n",
    "   *\"Top buyer / maximum purchase amount\"*,  \n",
    "   use this pattern to avoid grouping errors:\n",
    "   ```sql\n",
    "   SELECT \"Client Name\", \"Total Price (‚Çπ)\"\n",
    "   FROM orders\n",
    "   WHERE \"Total Price (‚Çπ)\" = (\n",
    "       SELECT MAX(\"Total Price (‚Çπ)\") FROM orders\n",
    "   );\n",
    "\"\"\"\n",
    "\n",
    "#  ROW-WISE LABELLED CHUNK GENERATION\n",
    "def generate_labelled_chunks(csv_path):\n",
    "    \"\"\"Creates labelled text chunks from each row for embeddings.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    chunks = []\n",
    "    for index, row in df.iterrows():\n",
    "        labelled_text = f\"Row ID: {index}\\n\"\n",
    "        for col in df.columns:\n",
    "            labelled_text += f\"{col}: {row[col]}\\n\"\n",
    "        chunks.append(labelled_text.strip())\n",
    "    return df, chunks\n",
    "\n",
    "\n",
    "df, labelled_chunks = generate_labelled_chunks(csv_path)\n",
    "\n",
    "documents = [Document(page_content=chunk) for chunk in labelled_chunks]\n",
    "\n",
    "print(f\" Generated {len(labelled_chunks)} labelled chunks for embeddings.\")\n",
    "\n",
    "#  Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "#  Create or load FAISS vector store\n",
    "if os.path.exists(faiss_index_path):\n",
    "    print(\" Loading existing FAISS index...\")\n",
    "    try:\n",
    "        vector_store = FAISS.load_local(\n",
    "            faiss_index_path, \n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\" FAISS index loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading FAISS index: {e}\")\n",
    "        print(\" Creating new FAISS index...\")\n",
    "        vector_store = FAISS.from_documents(documents, embeddings)\n",
    "        vector_store.save_local(faiss_index_path)\n",
    "        print(\" New FAISS index created and saved!\")\n",
    "else:\n",
    "    print(\" Creating new FAISS index...\")\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(faiss_index_path, exist_ok=True)\n",
    "    vector_store.save_local(faiss_index_path)\n",
    "    print(\" FAISS index created and saved!\")\n",
    "\n",
    "#  Set up retrievers\n",
    "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 15})\n",
    "keyword_retriever = BM25Retriever.from_documents(documents)\n",
    "keyword_retriever.k = 15\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, keyword_retriever],\n",
    "    weights=[0.6, 0.4]\n",
    ")\n",
    " \n",
    "class GraphStateRequired(TypedDict):\n",
    "    question: str  # Always required\n",
    "    \n",
    "class GraphState(GraphStateRequired, total=False):\n",
    "    intent: str\n",
    "    context: List[str]\n",
    "    answer: str\n",
    "    sql_query: str\n",
    "    validation_error: Optional[str]\n",
    "\n",
    "\n",
    "def intent_node(state: GraphState,config = None) -> GraphState:\n",
    "    \"\"\"Use LLM to classify query intent based on the Orders dataset\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    intent_prompt = f\"\"\"\n",
    "        You are an intent classifier for user questions over an **Orders dataset**.\n",
    "        The table has the following columns:\n",
    "        Order ID, Client Name, Email, Contact Number, Origin, Destination,\n",
    "        Product Name, Category, Material, Color, Quantity, Unit Price (‚Çπ),\n",
    "        Total Price (‚Çπ), Order Date, Delivery Date, Status.\n",
    "\n",
    "        RULE (important): If the user asks about *popularity, trends, likelihood, aesthetics, emotional value, suitability, or \"best suited\"*, treat the question as **semantic** ‚Äî even if the question mentions column names like Category or Product. Those words request interpretation, not a raw SQL lookup.\n",
    "\n",
    "        Your task is to classify the user's question into one of the following intents:\n",
    "\n",
    "        ---\n",
    "\n",
    "        1. **numeric** ‚Üí The question can be answered using structured, factual, or count-based data directly from the dataset.\n",
    "        Includes lookups, filters, conditions, or measurable aggregations.\n",
    "\n",
    "         Examples:\n",
    "        - \"How many orders are pending?\"\n",
    "        - \"Which category is ordered the most?\"\n",
    "        - \"List all clients whose orders are cancelled.\"\n",
    "        - \"Show orders where quantity > 10.\"\n",
    "        - \"Who bought d√©cor items?\"\n",
    "        - \"List clients who ordered furniture and curtains.\"\n",
    "        - \"Show total sales amount from Bangalore.\"\n",
    "        - \"Names of clients who placed multiple orders.\"\n",
    "        - \"List all clients whose orders are cancelled and who prefer d√©cor items.\"\n",
    "        - \"What is the origin of Kara Mata?\"    <-- entity attribute lookup ‚Üí numeric\n",
    "\n",
    "         NOTE: Entity attribute lookups (e.g., \"what is the product Jesse Williams ordered?\") count as **numeric**.\n",
    "\n",
    "        ---\n",
    "\n",
    "        2. **semantic** ‚Üí The question requires interpretation, reasoning, or subjective understanding\n",
    "        that cannot be directly derived from the dataset‚Äôs structured fields.\n",
    "        These questions involve opinions, trends, likelihood, aesthetics or suitability.\n",
    "\n",
    "         Examples:\n",
    "        - \"Which customers are likely to be loyal customers?\"\n",
    "        - \"Which products seem to be trending this month?\"\n",
    "        - \"Which items are best suited for festive seasons?\"\n",
    "        - \"Which products have emotional or aesthetic value?\"\n",
    "        - \"What categories are most popular among new customers?\"\n",
    "        - \"Which products are considered budget-friendly?\"\n",
    "        - \"Which customers might recommend our products?\"\n",
    "        - \"Which destinations appear to be popular among high-value clients?\"\n",
    "\n",
    "        KEY: If the phrasing contains words like *popular, trending, likely, seem, appear, best suited, emotional, aesthetic, preference, suitability* ‚Äî treat as **semantic**.\n",
    "\n",
    "        ---\n",
    "\n",
    "        3. **hybrid** ‚Üí The question contains both numeric and semantic parts.\n",
    "\n",
    "         Examples:\n",
    "        - \"What is the total sales amount, and which products are most popular in premium homes?\"\n",
    "        - \"Count d√©cor orders and explain which clients usually place them.\"\n",
    "\n",
    "        ---\n",
    "\n",
    "        4. **greet** ‚Üí Simple greetings (Hi, Hello, etc.)\n",
    "\n",
    "        ---\n",
    "\n",
    "        5. **ignore** ‚Üí Questions unrelated to the dataset.\n",
    "        Examples :\n",
    "        - \"Tell me a joke , Drop tables , Alter tables , Hi drop tables, modify , truncate etc\"\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Return only one word: numeric, semantic, hybrid, greet, or ignore.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    intent = llm.invoke(intent_prompt).content.strip().lower()\n",
    "    print(f\" Detected Intent: {intent}\")\n",
    "    state[\"intent\"] = intent\n",
    "    return state\n",
    "\n",
    "\n",
    "def greet_node(state: GraphState,config = None)-> GraphState:\n",
    "    state[\"answer\"] = \"Hello üëã! How can I assist you with the order data today?\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def ignore_node(state: GraphState , config = None) -> GraphState:\n",
    "    state[\"answer\"] = \"I'm designed to answer questions about the order dataset. Please ask something related.\"\n",
    "    return state\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def retriever_node(state: GraphState,config = None)-> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    try:\n",
    "        retrieved_chunks = hybrid_retriever.invoke(question)\n",
    "        context = \"\\n\".join([doc.page_content for doc in retrieved_chunks])\n",
    "        prompt = f\"\"\"\n",
    "        You are an analytical assistant answering questions based ONLY on the provided order dataset context.\n",
    "        Do NOT generate or imagine reasons, strategies, or external factors beyond what is visible in the context.\n",
    "\n",
    "        Context (from the dataset):\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer using patterns, trends, or comparisons visible within the dataset ONLY.\n",
    "        If the dataset does not contain explicit information to answer the question, say:\n",
    "        \"The dataset does not provide enough information to determine this.\"\n",
    "        Keep your response factual and concise.\n",
    "        \"\"\"\n",
    "        answer = llm.invoke(prompt).content.strip()\n",
    "\n",
    "       \n",
    "        state[\"answer\"] = answer\n",
    "    except Exception as e:\n",
    "        state[\"answer\"] = f\"Error using retriever: {e}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "VALID_COLUMNS = [\n",
    "    \"Order ID\", \"Client Name\", \"Email\", \"Contact Number\",\n",
    "    \"Origin\", \"Destination\", \"Product Name\", \"Category\",\n",
    "    \"Material\", \"Color\", \"Quantity\", \"Unit Price (‚Çπ)\",\n",
    "    \"Total Price (‚Çπ)\", \"Order Date\", \"Delivery Date\", \"Status\"\n",
    "]\n",
    "\n",
    "def sql_validator_node(state: dict):\n",
    "    \"\"\"Validates generated SQL to ensure it's safe and valid for DuckDB execution\"\"\"\n",
    "    sql_query = state.get(\"sql_query\", \"\").strip()\n",
    "    print(f\" Validating SQL query: {sql_query}\")\n",
    "\n",
    "    # 1. Ensure it's a SELECT query\n",
    "    if not sql_query.lower().lstrip().startswith(\"select\"):\n",
    "        state[\"validation_error\"] = \" Only SELECT queries are allowed.\"\n",
    "        return state\n",
    "\n",
    "    # 2. Block dangerous operations\n",
    "    forbidden_keywords = [\"insert\", \"update\", \"delete\", \"drop\", \"alter\", \"truncate\", \"create\"]\n",
    "    if any(kw in sql_query.lower() for kw in forbidden_keywords):\n",
    "        state[\"validation_error\"] = (\n",
    "            f\" Unsafe SQL operation detected. \"\n",
    "            f\"Keywords like {', '.join(forbidden_keywords)} are not allowed.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    # 3. Validate quoted column names used as actual columns (ignore aliases produced by AS)\n",
    "    quoted_names = re.findall(r'\"(.*?)\"', sql_query)\n",
    "    for name in quoted_names:\n",
    "        # if name exactly matches a real column, OK\n",
    "        if name in VALID_COLUMNS:\n",
    "            continue\n",
    "\n",
    "        # if the quoted name is used as an alias (AS \"name\"), skip validation\n",
    "        alias_pattern = re.search(r'\\bAS\\s+\"?' + re.escape(name) + r'\"?', sql_query, flags=re.IGNORECASE)\n",
    "        if alias_pattern:\n",
    "            # it's an alias; safe to ignore\n",
    "            continue\n",
    "\n",
    "        # otherwise it's an invalid column name\n",
    "        state[\"validation_error\"] = f\" Invalid column name used: '{name}'.\"\n",
    "        return state\n",
    "\n",
    "    # Passed all checks\n",
    "    state[\"validation_error\"] = None\n",
    "    print(\" SQL validation passed.\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def duckdb_node(state: GraphState,config = None)-> GraphState:\n",
    "    \"\"\"Handles numeric/structured questions ‚Äî validates SQL with SQLGlot before executing\"\"\"\n",
    "    query = state[\"question\"]\n",
    "\n",
    "    try:\n",
    "        #  Step 1: Ask LLM to generate SQL\n",
    "        sql_prompt = f\"{sql_system_prompt}\\nUser question: {query}\\nSQL:\"\n",
    "        sql_query = llm.invoke(sql_prompt).content.strip()\n",
    "\n",
    "        #  Step 2: Clean LLM formatting\n",
    "        sql_query = (\n",
    "            sql_query.replace(\"```sql\", \"\")\n",
    "                     .replace(\"```\", \"\")\n",
    "                     .replace(\"`\", \"\")\n",
    "                     .replace(\"SQL:\", \"\")\n",
    "                     .strip()\n",
    "        )\n",
    "\n",
    "        print(f\"\\n Generated SQL query:\\n{sql_query}\")\n",
    "        state[\"sql_query\"] = sql_query\n",
    "\n",
    "        #  Step 3: Syntax validation using SQLGlot\n",
    "        try:\n",
    "            parse_one(sql_query)\n",
    "            print(\" SQLGlot syntax check passed.\")\n",
    "        except Exception as parse_err:\n",
    "            state[\"answer\"] = f\"‚ö†Ô∏è SQL syntax error detected: {parse_err}\"\n",
    "            return state\n",
    "\n",
    "        #  Step 4: Custom SQL safety validation\n",
    "        validation_state = sql_validator_node(state)\n",
    "        if validation_state.get(\"validation_error\"):\n",
    "            state[\"answer\"] = validation_state[\"validation_error\"]\n",
    "            return state\n",
    "\n",
    "        #  Step 5: Safe execution inside a local DuckDB context\n",
    "        try:\n",
    "            with duckdb.connect(db_path) as con:\n",
    "                result_df = con.execute(sql_query).fetchdf()\n",
    "\n",
    "        except Exception as exec_err:\n",
    "            state[\"answer\"] = f\"‚ö†Ô∏è SQL execution failed: {exec_err}\"\n",
    "            return state\n",
    "\n",
    "        if result_df.empty:\n",
    "            state[\"answer\"] = \"No matching records found.\"\n",
    "            return state\n",
    "\n",
    "        #  Step 6: Convert results to plain text\n",
    "        result_text = result_df.to_string(index=False)\n",
    "\n",
    "        #  Step 7: Generate human-readable summary\n",
    "        summary_prompt = f\"\"\"\n",
    "        The user asked: {query}\n",
    "        The SQL result is:\n",
    "        {result_text}\n",
    "\n",
    "        Write a natural, clear explanation of these results.\n",
    "        Avoid skipping rows or making assumptions.\n",
    "        \"\"\"\n",
    "        answer = llm.invoke(summary_prompt).content.strip()\n",
    "        state[\"answer\"] = answer\n",
    "\n",
    "    except Exception as e:\n",
    "        state[\"answer\"] = f\"Error executing SQL: {str(e)}\"\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def hybrid_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    Handles hybrid, multi-intent queries that may contain:\n",
    "      - multiple numeric sub-queries (SQL-based)\n",
    "      - multiple semantic sub-queries (LLM-based)\n",
    "      - or a mix of both.\n",
    "    Ensures separate execution and clear structured output.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(f\"\\n [Hybrid Node] Received question ‚Üí {question}\")\n",
    "\n",
    "    # 1Ô∏è SPLITTING PHASE ‚Äî Identify all numeric & semantic sub-questions\n",
    "    \n",
    "    split_prompt = f\"\"\"\n",
    "    You are a professional query decomposition assistant.\n",
    "    Split the following user question into atomic sub-questions.\n",
    "\n",
    "    Each sub-question should be labeled as:\n",
    "      - numeric ‚Üí if it can be answered using SQL filters, counts, or aggregates (explicit SQL lookups or attribute lookups e.g., \"what is the product Jesse Williams ordered?\")\n",
    "      - semantic ‚Üí if it requires descriptive, interpretive, or trend-based reasoning (popularity, trending, suitability, emotional/aesthetic value).\n",
    "\n",
    "    Important rule: If a sub-question contains words like\n",
    "    [\"popular\", \"trending\", \"likely\", \"seem\", \"appear\", \"best suited\", \"emotional\", \"aesthetic\", \"budget-friendly\", \"suitable for\", \"preference\", \"preference for\", \"trend\"],\n",
    "    classify that sub-question as **semantic** (these require interpretation), even if they mention columns like Category or Product.\n",
    "\n",
    "    Return JSON strictly in this format:\n",
    "    {{\n",
    "      \"numeric_parts\": [ \"...\" ],\n",
    "      \"semantic_parts\": [ \"...\" ],\n",
    "      \"dependent\": true/false\n",
    "    }}\n",
    "\n",
    "    Examples:\n",
    "    1. \"List all clients whose orders are cancelled and list the clients who prefer decor items\"\n",
    "    ‚Üí {{\n",
    "      \"numeric_parts\": [\n",
    "        \"List all clients whose orders are cancelled\",\n",
    "        \"List all clients who prefer decor items\"\n",
    "      ],\n",
    "      \"semantic_parts\": [],\n",
    "      \"dependent\": false\n",
    "    }}\n",
    "\n",
    "    2. \"What categories are most popular among new customers and Which products have emotional or aesthetic value?\"\n",
    "    ‚Üí {{\n",
    "      \"numeric_parts\": [],\n",
    "      \"semantic_parts\": [\n",
    "        \"What categories are most popular among new customers\",\n",
    "        \"Which products have emotional or aesthetic value\"\n",
    "      ],\n",
    "      \"dependent\": false\n",
    "    }}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        split_result = llm.invoke(split_prompt).content\n",
    "        split_result = split_result.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        print(f\" Raw Split Result: {split_result}\")\n",
    "        split_data = json.loads(split_result)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing split result: {e}\")\n",
    "        split_data = {\"numeric_parts\": [], \"semantic_parts\": [], \"dependent\": False}\n",
    "\n",
    "    numeric_parts = split_data.get(\"numeric_parts\", [])\n",
    "    semantic_parts = split_data.get(\"semantic_parts\", [])\n",
    "    dependent = split_data.get(\"dependent\", False)\n",
    "\n",
    "    print(f\" Parsed numeric part(s): {numeric_parts}\")\n",
    "    print(f\" Parsed semantic part(s): {semantic_parts}\")\n",
    "    print(f\" Dependency flag: {dependent}\")\n",
    "\n",
    "    # 2Ô∏è EXECUTE NUMERIC SUB-QUERIES\n",
    "    numeric_results = []\n",
    "    if numeric_parts:\n",
    "        print(f\" Executing {len(numeric_parts)} numeric subquery(ies)...\")\n",
    "        for i, sub_q in enumerate(numeric_parts, 1):\n",
    "            print(f\"\\n Numeric Sub-query {i}: {sub_q}\")\n",
    "            temp_state = {\"question\": sub_q, \"intent\": \"numeric\", \"context\": [], \"answer\": \"\"}\n",
    "            try:\n",
    "                numeric_state = duckdb_node(temp_state)\n",
    "                result = numeric_state.get(\"answer\", \"\")\n",
    "                numeric_results.append({\n",
    "                    \"subquery\": sub_q,\n",
    "                    \"result\": result\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\" Error in numeric sub-query {i}: {e}\")\n",
    "\n",
    "    # 3Ô∏è EXECUTE SEMANTIC SUB-QUERIES\n",
    "    semantic_results = []\n",
    "    if semantic_parts:\n",
    "        print(f\" Executing {len(semantic_parts)} semantic subquery(ies)...\")\n",
    "        for i, sub_q in enumerate(semantic_parts, 1):\n",
    "            print(f\"\\n Semantic Sub-query {i}: {sub_q}\")\n",
    "            temp_state = {\"question\": sub_q, \"intent\": \"semantic\", \"context\": [], \"answer\": \"\"}\n",
    "            try:\n",
    "                semantic_state = retriever_node(temp_state)\n",
    "                result = semantic_state.get(\"answer\", \"\")\n",
    "                semantic_results.append({\n",
    "                    \"subquery\": sub_q,\n",
    "                    \"result\": result\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\" Error in semantic sub-query {i}: {e}\")\n",
    "\n",
    "    # 4Ô∏è FORMAT & MERGE RESULTS CLEARLY\n",
    "    combined_output = []\n",
    "\n",
    "    if numeric_results:\n",
    "        for i, entry in enumerate(numeric_results, 1):\n",
    "            combined_output.append(\n",
    "                f\" **Numeric Result {i}:** {entry['subquery']}\\n{entry['result']}\\n\"\n",
    "            )\n",
    "\n",
    "    if semantic_results:\n",
    "        for i, entry in enumerate(semantic_results, 1):\n",
    "            combined_output.append(\n",
    "                f\" **Semantic Result {i}:** {entry['subquery']}\\n{entry['result']}\\n\"\n",
    "            )\n",
    "\n",
    "    if not combined_output:\n",
    "        combined_output = [\"‚ö†Ô∏è No valid results found for this query.\"]\n",
    "\n",
    "    final_answer = \"\\n\".join(combined_output)\n",
    "    # print(f\"\\n Final Combined Answer:\\n{final_answer}\")\n",
    "\n",
    "    # Store cleanly into state\n",
    "    state[\"answer\"] = final_answer\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"intent\", intent_node)\n",
    "graph.add_node(\"greet\", greet_node)\n",
    "graph.add_node(\"ignore\", ignore_node)\n",
    "graph.add_node(\"duckdb\", duckdb_node)\n",
    "graph.add_node(\"retriever\", retriever_node)\n",
    "graph.add_node(\"hybrid\", hybrid_node)\n",
    "\n",
    "graph.set_entry_point(\"intent\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"intent\",\n",
    "    lambda state: state[\"intent\"],\n",
    "    {\n",
    "        \"greet\": \"greet\",\n",
    "        \"ignore\": \"ignore\",\n",
    "        \"numeric\": \"duckdb\",\n",
    "        \"semantic\": \"retriever\",\n",
    "        \"hybrid\": \"hybrid\",\n",
    "    },\n",
    ")\n",
    "\n",
    "graph.add_edge(\"greet\", END)\n",
    "graph.add_edge(\"ignore\", END)\n",
    "graph.add_edge(\"duckdb\", END)\n",
    "graph.add_edge(\"retriever\", END)\n",
    "graph.add_edge(\"hybrid\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n Smart Query Assistant ready! Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            print(f\"You: {user_input}\")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"Assistant: Goodbye !\")\n",
    "                break\n",
    "\n",
    "            result = app.invoke({\"question\": user_input})\n",
    "            print(f\"Assistant: {result['answer']}\\n\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nAssistant: Goodbye ! \")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {str(e)}\")\n",
    "            print(\"Please try again with a different question.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
