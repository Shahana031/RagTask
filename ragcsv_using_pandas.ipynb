{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_experimental.agents import create_pandas_dataframe_agent\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"students.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c895b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    question : str\n",
    "    intent : str\n",
    "    context : List[str]\n",
    "    answer:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=csv_path)\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 500,chunk_overlap = 50)\n",
    "texts = splitter.split_documents(documents)\n",
    "print(f\"Total chunks created:¬†{len(texts)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "llm  = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "persist_directory = r\"D:\\RAG Task\"\n",
    "collection_name =\"article_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bcb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=texts,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\":4})\n",
    "keyword_retriever=BM25Retriever.from_documents(texts)\n",
    "\n",
    "hybrid_retriever=EnsembleRetriever(\n",
    "    retrievers=[vector_retriever,keyword_retriever],\n",
    "    weights=[0.6,0.4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents import create_pandas_dataframe_agent\n",
    "\n",
    "pandas_agent = create_pandas_dataframe_agent(\n",
    "    llm,\n",
    "    df,\n",
    "    verbose=True,\n",
    "    allow_dangerous_code=True  # üëà this is required\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_node(state : GraphState):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    classification_prompt = f\"\"\"\n",
    "    \n",
    "    Classify the following question into one of these categories:\n",
    "    1. greeting\n",
    "    2. relevant_to_document\n",
    "    3. irrelevant\n",
    "    \n",
    "    Question : \"{question}\"\n",
    "    \n",
    "    Reply only with the category name.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = llm.invoke(classification_prompt).content.strip().lower()\n",
    "    \n",
    "    return {\"intent\" : result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet_node(state:GraphState):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a friendly AI assistant.\n",
    "    The user greeted you with: \"{question}\".\n",
    "    Respond politely and naturally, as if you are having a short chat before helping with their document.\n",
    "    Example responses: \"Hey there! How‚Äôs it going?\" or \"Hello! How can I assist you today?\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8eb851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_node(state:GraphState):\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant specialized in answering questions related only to a provided document.\n",
    "    The user asked: \"{question}\".\n",
    "    Politely tell the user that you can only answer questions related to the document content.\n",
    "    Example responses:\n",
    "    - \"I'm sorry, I can only help with questions about the uploaded document.\"\n",
    "    - \"That seems unrelated to the document. Could you please ask something based on it?\"\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\":response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdac08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retriever_node(state: GraphState):\n",
    "#     question = state[\"question\"]\n",
    "\n",
    "    \n",
    "#     hybrid_docs = hybrid_retriever.invoke(question)\n",
    "\n",
    "    \n",
    "#     pairs = [(question, doc.page_content) for doc in hybrid_docs]\n",
    "\n",
    "    \n",
    "#     scores = reranker.predict(pairs)\n",
    "\n",
    "    \n",
    "#     ranked_docs = [doc for _, doc in sorted(zip(scores, hybrid_docs), key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "    \n",
    "#     top_docs = ranked_docs[:3]\n",
    "\n",
    "  \n",
    "#     context = [doc.page_content for doc in top_docs]\n",
    "\n",
    "#     return {\"context\": context}\n",
    "\n",
    "def retriever_node(state: GraphState):\n",
    "    \"\"\"Retrieve and re-rank documents\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieve hybrid docs\n",
    "    hybrid_docs = hybrid_retriever.invoke(question)\n",
    "\n",
    "    # Pair question with retrieved docs for reranking\n",
    "    pairs = [(question, doc.page_content) for doc in hybrid_docs]\n",
    "\n",
    "    # Rerank\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked_docs = [doc for _, doc in sorted(zip(scores, hybrid_docs), key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "    # Select top 3 docs\n",
    "    top_docs = ranked_docs[:3]\n",
    "    context = [doc.page_content for doc in top_docs]\n",
    "\n",
    "    # Store in state for next node\n",
    "    state[\"context\"] = context\n",
    "    return state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def answer_node(state: GraphState):\n",
    "#     question = state.get(\"question\", \"\")\n",
    "#     context = state.get(\"context\", \"\")\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a helpful AI assistant. Use the context below to answer the user's question.\n",
    "\n",
    "#     Context:\n",
    "#     {context}\n",
    "\n",
    "#     Question:\n",
    "#     {question}\n",
    "\n",
    "#     If the answer is not found in the context, say \"I'm not sure based on the available information.\"\n",
    "#     \"\"\"\n",
    "\n",
    "#     response = llm.invoke(prompt)\n",
    "#     return {\"answer\": response.content}\n",
    "\n",
    "def answer_node(state: GraphState):\n",
    "    \"\"\"Answer using RAG or Pandas Agent\"\"\"\n",
    "    query = state[\"question\"]\n",
    "\n",
    "    # Simple heuristic: numeric/structured ‚Üí Pandas Agent\n",
    "    numeric_keywords = [\"how many\", \"average\", \"count\", \"sum\", \"highest\", \"lowest\", \"top\", \"less\", \"greater\", \"equal\", \"mean\", \"min\", \"max\", \"gpa\", \"department\"]\n",
    "\n",
    "    if any(kw in query.lower() for kw in numeric_keywords):\n",
    "        print(\" Routing to Pandas Agent\")\n",
    "        try:\n",
    "            answer = pandas_agent.run(query)\n",
    "        except Exception as e:\n",
    "            answer = f\"Error using Pandas Agent: {str(e)}\"\n",
    "    else:\n",
    "        print(\"üìö Using retrieved context\")\n",
    "        context = \"\\n\".join(state.get(\"context\", []))\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer briefly:\"\n",
    "        try:\n",
    "            answer = llm.invoke(prompt).content\n",
    "        except Exception as e:\n",
    "            answer = f\"Error using RAG: {str(e)}\"\n",
    "\n",
    "    state[\"answer\"] = answer\n",
    "    return state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07918454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_from_intent(state:GraphState):\n",
    "    intent = state[\"intent\"].lower()\n",
    "    \n",
    "    if \"greeting\" in intent:\n",
    "        return \"greet\"\n",
    "    elif \"relevant\" in intent:\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        return \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87777a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(GraphState)\n",
    "\n",
    "\n",
    "graph.add_node(\"intent\", intent_node)\n",
    "graph.add_node(\"greet\", greet_node)\n",
    "graph.add_node(\"ignore\", ignore_node)\n",
    "graph.add_node(\"retrieve\", retriever_node)\n",
    "graph.add_node(\"answer\", answer_node)\n",
    "\n",
    "graph.add_edge(START, \"intent\")\n",
    "graph.add_conditional_edges(\"intent\", route_from_intent, [\"greet\", \"retrieve\", \"ignore\"])\n",
    "graph.add_edge(\"retrieve\", \"answer\")\n",
    "\n",
    "\n",
    "graph.add_edge(\"greet\", END)\n",
    "graph.add_edge(\"ignore\", END)\n",
    "graph.add_edge(\"answer\", END)\n",
    "\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AI: RAG Assistant is ready! Type 'exit' or 'quit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print(f\"You :{user_input}\")\n",
    "    \n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Assistant: Goodbye! \")\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        response =response = app.invoke({\"question\": user_input})\n",
    "        print(\"Assistant:\", response.get(\"answer\", \"No response generated.\"))\n",
    "    except Exception as e:\n",
    "        print(\"‚ö† Error:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44112d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DuckDB table 'students' loaded directly from CSV!\n",
      "You :what is the feedback for Akshay rao and count the number of students?\n",
      " Detected Intent: hybrid\n",
      " Split result: {\n",
      "    \"numeric\": \"count the number of students\",\n",
      "    \"semantic\": \"what is the feedback for Akshay Rao\"\n",
      "}\n",
      " Sending numeric part to duckdb_node: count the number of students\n",
      " SQL query: SELECT COUNT(*) FROM students;\n",
      " Sending semantic part to retriever_node: what is the feedback for Akshay Rao\n",
      " Final answer: The feedback for Akshay Rao indicates that he requires improvement in report presentation. Additionally, there are a total of 50 students.\n",
      "Assistant: The feedback for Akshay Rao indicates that he requires improvement in report presentation. Additionally, there are a total of 50 students.\n",
      "You :exit\n",
      "Assistant: Goodbye! \n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict,List\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "csv_path = \"students.csv\"\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "#with this ‚Äî DuckDB reads CSV natively\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE students AS\n",
    "    SELECT * FROM read_csv_auto('{csv_path}', header=True)\n",
    "\"\"\")\n",
    "\n",
    "print(\" DuckDB table 'students' loaded directly from CSV!\")\n",
    "\n",
    "\n",
    "\n",
    "sql_system_prompt = \"\"\"\n",
    "You are a SQL expert helping to query a DuckDB table named `students`.\n",
    "\n",
    "Table name: students  \n",
    "Columns and their meanings:\n",
    "- StudentID: Unique ID of the student (integer)\n",
    "- Name: Student's full name (text)\n",
    "- Department: Department of study (e.g., Computer Science, Electrical, Mechanical)\n",
    "- Grade: Academic grade (A+, A, A-, B+, etc.)\n",
    "- GPA: Grade Point Average (numeric, e.g., 3.8)\n",
    "- Feedback: Text description of the student's performance and expertise.\n",
    "\n",
    "Sample data:\n",
    "1 | Alice | Computer Science | A | 3.9 | Excellent in IoT and AI projects\n",
    "2 | Bob | Electrical | B+ | 3.4 | Good in circuit design and teamwork\n",
    "3 | Carol | Mechanical | A- | 3.7 | Great at robotics and embedded systems\n",
    "\n",
    "----------------------------------\n",
    "INSTRUCTIONS:\n",
    "----------------------------------\n",
    "1. Generate SQL queries **only** for numeric or structured filters.  \n",
    "   Examples:\n",
    "   - GPA > 3.8  \n",
    "   - Grade = 'A'  \n",
    "   - Department = 'Computer Science' when asked to list the student's department\n",
    "\n",
    "2. **Do NOT** generate or include text-based or descriptive filters  \n",
    "   such as expertise, feedback content, interests, or skills (e.g., ‚ÄúIoT‚Äù, ‚ÄúAI‚Äù, ‚Äúleadership‚Äù).  \n",
    "   Those are handled separately by another retriever system \n",
    "\n",
    "3. Use the correct table name `students` and column names exactly as given.\n",
    "\n",
    "4. Never hallucinate new columns or tables.\n",
    "\n",
    "5. Return only the SQL query ‚Äî no markdown, explanations, or additional commentary.\n",
    "\n",
    "\n",
    "\n",
    "Example valid queries:\n",
    "- SELECT Name, GPA FROM students WHERE GPA > 3.8;\n",
    "- SELECT * FROM students WHERE Department = 'Computer Science' AND Grade = 'A';\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "loader = CSVLoader(file_path=csv_path, encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question : str\n",
    "    intent : str\n",
    "    context : List[str]\n",
    "    answer:str\n",
    "   \n",
    "   \n",
    "def intent_node(state: GraphState):\n",
    "    \"\"\"Use LLM to classify query intent\"\"\"\n",
    "    query = state[\"question\"]\n",
    "    intent_prompt = f\"\"\"\n",
    "        You are an intent classifier for user questions over a student dataset.\n",
    "    The table contains columns: StudentID, Name, Department, Grade, GPA, Feedback.\n",
    "\n",
    "    Classify the intent of the question as one of the following:\n",
    "\n",
    "    1. \"numeric\" ‚Üí if it involves:\n",
    "    - filters or comparisons on structured fields like Department, Grade, GPA, or StudentID\n",
    "    - examples: \"List students in Computer Science\", \"Show students with Grade A\", \"Who has GPA > 3.5\"\n",
    "\n",
    "    2. \"semantic\" ‚Üí if it involves open-ended descriptions, expertise, or meanings inside text fields like Feedback\n",
    "    - examples: \"Who is good at AI?\", \"Which student has leadership skills?\"\n",
    "\n",
    "    3. \"hybrid\" ‚Üí if it mixes both structured filters and descriptive parts\n",
    "    - example: \"List students in Computer Science who are good at AI\"\n",
    "\n",
    "    4. \"greet\" ‚Üí greetings like \"Hi\", \"Hello\"\n",
    "\n",
    "    5. \"ignore\" ‚Üí if it's unrelated to student data\n",
    "\n",
    "    Question: {query}\n",
    "    Return only one word : : numeric, semantic, hybrid, greet, or ignore.\n",
    "    \"\"\"\n",
    "    intent = llm.invoke(intent_prompt).content.strip().lower()\n",
    "    print(f\" Detected Intent: {intent}\")\n",
    "    state[\"intent\"] = intent\n",
    "    return state\n",
    "\n",
    "\n",
    "def greet_node(state: GraphState):\n",
    "    state[\"answer\"] = \"Hello! üëã How can I assist you with the student data today?\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def ignore_node(state: GraphState):\n",
    "    state[\"answer\"] = \"I'm here to answer questions about the student dataset. Could you ask something related to that?\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def duckdb_node(state: GraphState):\n",
    "    \"\"\"Numeric / structured question handler with natural output\"\"\"\n",
    "    query = state[\"question\"]\n",
    "    try:\n",
    "        sql_prompt = f\"{sql_system_prompt}\\nUser question: {query}\\nSQL:\"\n",
    "        sql_query = llm.invoke(sql_prompt).content.strip()\n",
    "\n",
    "        # Clean SQL\n",
    "        sql_query = (\n",
    "            sql_query.replace(\"```sql\", \"\")\n",
    "            .replace(\"```\", \"\")\n",
    "            .replace(\"`\", \"\")\n",
    "            .replace(\"SQL:\", \"\")\n",
    "            .strip()\n",
    "        )\n",
    "\n",
    "        print(f\" SQL query: {sql_query}\")\n",
    "\n",
    "        # Execute SQL on DuckDB\n",
    "        result_df = con.execute(sql_query).fetchdf()\n",
    "\n",
    "        if result_df.empty:\n",
    "            state[\"answer\"] = \"No matching records found.\"\n",
    "            return state\n",
    "\n",
    "        # Convert all rows to text\n",
    "        result_text = result_df.to_string(index=False)\n",
    "\n",
    "        # LLM to summarize results naturally\n",
    "        summary_prompt = f\"\"\"\n",
    "        The user asked: {query}\n",
    "        The SQL result is:\n",
    "        {result_text}\n",
    "\n",
    "        Write a clear and complete natural language response that lists all relevant names or details.\n",
    "        Do not skip or summarize results.\n",
    "        \"\"\"\n",
    "        answer = llm.invoke(summary_prompt).content.strip()\n",
    "\n",
    "        state[\"answer\"] = answer\n",
    "\n",
    "    except Exception as e:\n",
    "        state[\"answer\"] = f\"Error executing SQL: {str(e)}\"\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def retriever_node(state: GraphState):\n",
    "    \"\"\"Semantic / descriptive question handler\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    try:\n",
    "        retrieved_chunks = retriever.invoke(question)\n",
    "        pairs = [(question, doc.page_content) for doc in retrieved_chunks]\n",
    "       \n",
    "        top_docs = retrieved_chunks[:3]\n",
    "        context = \"\\n\".join([doc.page_content for doc in top_docs])\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer briefly:\"\n",
    "        answer = llm.invoke(prompt).content\n",
    "        state[\"answer\"] = answer\n",
    "    except Exception as e:\n",
    "        state[\"answer\"] = f\"Error using retriever: {str(e)}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def hybrid_node(state: GraphState):\n",
    "    \"\"\"Handles hybrid queries by splitting into numeric & semantic sub-questions\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    try:\n",
    "        # Step 1Ô∏è: Ask LLM to split the hybrid query into numeric and semantic subparts\n",
    "        split_prompt = f\"\"\"\n",
    "            Split the user query into numeric and semantic parts.\n",
    "        - Numeric parts are those answerable via SQL (count, average, filter, etc.).\n",
    "        - Semantic parts are descriptive (skills, feedback, comments, etc.).\n",
    "        - If there are multiple sub-questions, list them in an array.\n",
    "\n",
    "        Return clean JSON like:\n",
    "        {{\n",
    "            \"numeric\": \"subquestion for numeric logic\",\n",
    "            \"semantic\": \"subquestion for semantic logic\"\n",
    "        }}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "\n",
    "        split_result = llm.invoke(split_prompt).content.strip()\n",
    "        print(\" Split result:\", split_result)\n",
    "\n",
    "        import json\n",
    "        try:\n",
    "            #  Clean any code block wrappers before parsing\n",
    "            split_result = (\n",
    "                split_result.replace(\"```json\", \"\")\n",
    "                            .replace(\"```\", \"\")\n",
    "                            .strip()\n",
    "            )\n",
    "\n",
    "            parsed = json.loads(split_result)\n",
    "            numeric_part = parsed.get(\"numeric\", \"\").strip()\n",
    "            semantic_part = parsed.get(\"semantic\", \"\").strip()\n",
    "        except json.JSONDecodeError:\n",
    "            numeric_part = \"\"\n",
    "            semantic_part = \"\"\n",
    "            print(\" LLM didn't return valid JSON ‚Äî skipping split.\")\n",
    "\n",
    "        # Step 2Ô∏è: Route the numeric part to duckdb_node\n",
    "        numeric_answer = \"\"\n",
    "        if numeric_part:\n",
    "            print(f\" Sending numeric part to duckdb_node: {numeric_part}\")\n",
    "            temp_state = {\"question\": numeric_part}\n",
    "            temp_state = {\n",
    "                \"question\": numeric_part,\n",
    "                \"inttent\": \"\",\n",
    "                \"context\": [],\n",
    "                \"answer\": \"\"\n",
    "        }\n",
    "            numeric_state = duckdb_node(temp_state)\n",
    "            numeric_answer = numeric_state.get(\"answer\", \"\")\n",
    "\n",
    "        # Step 3Ô∏è: Route the semantic part to retriever_node\n",
    "        semantic_answer = \"\"\n",
    "        if semantic_part:\n",
    "            print(f\" Sending semantic part to retriever_node: {semantic_part}\")\n",
    "            temp_state = {\"question\": semantic_part}\n",
    "            semantic_state = retriever_node(temp_state)\n",
    "            semantic_answer = semantic_state.get(\"answer\", \"\")\n",
    "\n",
    "        # Step 4Ô∏è: Merge both results using LLM\n",
    "        combine_prompt = f\"\"\"\n",
    "        The user originally asked: {question}\n",
    "\n",
    "        Numeric insight:\n",
    "        {numeric_answer}\n",
    "\n",
    "        Semantic insight:\n",
    "        {semantic_answer}\n",
    "\n",
    "        Combine these into a single, clear and concise final answer.\n",
    "        \"\"\"\n",
    "        final_answer = llm.invoke(combine_prompt).content.strip()\n",
    "        print(\" Final answer:\", final_answer)\n",
    "\n",
    "        state[\"answer\"] = final_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        state[\"answer\"] = f\"Error in hybrid node: {str(e)}\"\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "graph.add_node(\"intent\", intent_node)\n",
    "graph.add_node(\"greet\", greet_node)\n",
    "graph.add_node(\"ignore\", ignore_node)\n",
    "graph.add_node(\"duckdb\", duckdb_node)\n",
    "graph.add_node(\"retriever\", retriever_node)\n",
    "graph.add_node(\"hybrid\", hybrid_node)\n",
    "\n",
    "graph.set_entry_point(\"intent\")\n",
    "\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"intent\",\n",
    "    lambda state: state[\"intent\"],\n",
    "    {\n",
    "        \"greet\": \"greet\",\n",
    "        \"ignore\": \"ignore\",\n",
    "        \"numeric\": \"duckdb\",\n",
    "        \"semantic\": \"retriever\",\n",
    "        \"hybrid\": \"hybrid\",\n",
    "        \"answer\": \"retriever\"  \n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "graph.add_edge(\"greet\", END)\n",
    "graph.add_edge(\"ignore\", END)\n",
    "graph.add_edge(\"duckdb\", END)\n",
    "graph.add_edge(\"retriever\", END)\n",
    "graph.add_edge(\"hybrid\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        \n",
    "        print(f\"You :{user_input}\")\n",
    "    \n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Assistant: Goodbye! \")\n",
    "            break\n",
    "\n",
    "        result = app.invoke({\"question\": user_input})\n",
    "        print(f\"Assistant: {result['answer']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
