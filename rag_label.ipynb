{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a851dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:59: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\R'\n",
      "C:\\Users\\shanu\\AppData\\Local\\Temp\\ipykernel_30592\\130187166.py:59: SyntaxWarning: invalid escape sequence '\\R'\n",
      "  persist_directory = \"D:\\RAG Task\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to DuckDB through SQLAlchemy and created table 'orders'\n",
      "‚úÖ Generated 70 labelled chunks for embeddings.\n",
      "\n",
      "üöÄ Smart Query Assistant ready! Type 'exit' to quit.\n",
      "\n",
      "You : how many products in total?\n",
      "üéØ Detected intent ‚Üí numeric\n",
      "\n",
      "üßÆ [DuckDB Node] Executing for ‚Üí how many products in total?\n",
      "\n",
      "üß† [SQL Builder] Received question ‚Üí how many products in total?\n",
      "‚ö†Ô∏è [SQL Builder] No rule matched for this query.\n",
      "üß± [SQL Builder] Generated SQL ‚Üí None\n",
      "‚ö†Ô∏è [DuckDB Node] Error executing SQL: SQL Builder returned None.\n",
      "Assistant: Error executing SQL: SQL Builder returned None.\n",
      "\n",
      "You : who exit\n",
      "üéØ Detected intent ‚Üí ignore\n",
      "Assistant: I'm designed to answer questions about the order dataset. Please ask something related.\n",
      "\n",
      "You : exit\n",
      "Assistant: Goodbye üëã\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever,EnsembleRetriever\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain_core.documents import Document \n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# ==========================================================\n",
    "# 1Ô∏è‚É£  CSV LOADING + DUCKDB INITIALIZATION\n",
    "# ==========================================================\n",
    "CSV_PATH = \"Client_Shipment_Orders.csv\"\n",
    "\n",
    "## Load your dataset\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Create SQLAlchemy engine for DuckDB (in-memory)\n",
    "engine = create_engine(\"duckdb:///:memory:\")\n",
    "\n",
    "# Get DuckDB native connection from SQLAlchemy\n",
    "conn = engine.raw_connection().driver_connection\n",
    "\n",
    "# Register your DataFrame as a temporary DuckDB table\n",
    "conn.register(\"orders_df\", df)\n",
    "\n",
    "# Create a permanent DuckDB table from it\n",
    "conn.execute(\"CREATE TABLE orders AS SELECT * FROM orders_df\")\n",
    "\n",
    "print(\"‚úÖ Connected to DuckDB through SQLAlchemy and created table 'orders'\")\n",
    "\n",
    "# ==========================================================\n",
    "# 2Ô∏è‚É£  ROW-WISE LABELLED CHUNK GENERATION\n",
    "# ==========================================================\n",
    "def generate_labelled_chunks(csv_path):\n",
    "    \"\"\"Creates labelled text chunks from each row for embeddings.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    chunks = []\n",
    "    for index, row in df.iterrows():\n",
    "        labelled_text = f\"Row ID: {index}\\n\"\n",
    "        for col in df.columns:\n",
    "            labelled_text += f\"{col}: {row[col]}\\n\"\n",
    "        chunks.append(labelled_text.strip())\n",
    "    return df, chunks\n",
    "\n",
    "\n",
    "df, labelled_chunks = generate_labelled_chunks(CSV_PATH)\n",
    "\n",
    "documents = [Document(page_content=chunk) for chunk in labelled_chunks]\n",
    "\n",
    "print(f\"‚úÖ Generated {len(labelled_chunks)} labelled chunks for embeddings.\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3Ô∏è‚É£  EMBEDDING + VECTORSTORE INITIALIZATION\n",
    "# ==========================================================\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "persist_directory = \"D:\\RAG Task\"\n",
    "collection_name = \"shipment_orders\"\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=collection_name,\n",
    ")\n",
    "\n",
    "\n",
    "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "keyword_retriever = BM25Retriever.from_documents(documents)\n",
    "keyword_retriever.k = 4\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers= [vector_retriever,keyword_retriever],\n",
    "    weights=[0.6,0.4]\n",
    ")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 4Ô∏è‚É£  SQL BUILDER ‚Äî RULE BASED (SAFE)\n",
    "# ==========================================================\n",
    "def build_sql_query(question: str) -> str:\n",
    "    q = question.lower().strip()\n",
    "    print(f\"\\nüß† [SQL Builder] Received question ‚Üí {q}\")\n",
    "\n",
    "    if \"how many\" in q and \"client\" in q:\n",
    "        sql = 'SELECT COUNT(DISTINCT \"Client Name\") AS total_clients FROM orders;'\n",
    "    elif \"total sales\" in q or \"total price\" in q:\n",
    "        sql = 'SELECT SUM(\"Total Price (‚Çπ)\") AS total_sales FROM orders;'\n",
    "    elif \"average\" in q or \"avg price\" in q:\n",
    "        sql = 'SELECT AVG(\"Unit Price (‚Çπ)\") AS avg_unit_price FROM orders;'\n",
    "    elif \"total quantity\" in q:\n",
    "        sql = 'SELECT SUM(\"Quantity\") AS total_quantity FROM orders;'\n",
    "    elif \"dining table\" in q:\n",
    "        sql = 'SELECT COUNT(*) AS dining_table_orders FROM orders WHERE LOWER(\"Product Name\") LIKE \\'%dining table%\\';'\n",
    "    elif \"pending\" in q:\n",
    "        sql = 'SELECT COUNT(*) AS pending_orders FROM orders WHERE LOWER(Status) = \\'pending\\';'\n",
    "    elif \"delivered\" in q:\n",
    "        sql = 'SELECT COUNT(*) AS delivered_orders FROM orders WHERE LOWER(Status) = \\'delivered\\';'\n",
    "    else:\n",
    "        sql = None\n",
    "        print(\"‚ö†Ô∏è [SQL Builder] No rule matched for this query.\")\n",
    "\n",
    "    print(f\"üß± [SQL Builder] Generated SQL ‚Üí {sql}\")\n",
    "    return sql\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 5Ô∏è‚É£  DUCKDB NODE ‚Äî EXECUTES QUERIES SAFELY\n",
    "# ==========================================================\n",
    "def duckdb_node(state: dict) -> dict:\n",
    "    try:\n",
    "        question = state.get(\"question\", \"\")\n",
    "        print(f\"\\nüßÆ [DuckDB Node] Executing for ‚Üí {question}\")\n",
    "\n",
    "        sql_query = build_sql_query(question)\n",
    "        if not sql_query:\n",
    "            raise ValueError(\"SQL Builder returned None.\")\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(sql_query))\n",
    "            rows = result.fetchall()\n",
    "\n",
    "        if not rows:\n",
    "            answer = \"No matching records found.\"\n",
    "        else:\n",
    "            answer = str(rows[0])\n",
    "\n",
    "        print(f\"‚úÖ [DuckDB Node] Query executed successfully ‚Üí {answer}\")\n",
    "        state[\"answer\"] = answer\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"Error executing SQL: {e}\"\n",
    "        print(f\"‚ö†Ô∏è [DuckDB Node] {err}\")\n",
    "        state[\"answer\"] = err\n",
    "        return state\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 6Ô∏è‚É£  RETRIEVER NODE ‚Äî HANDLES SEMANTIC QUERIES\n",
    "# ==========================================================\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def retriever_node(state: dict):\n",
    "    question = state[\"question\"]\n",
    "    try:\n",
    "        retrieved_chunks = hybrid_retriever.invoke(question)\n",
    "        context = \"\\n\".join([doc.page_content for doc in retrieved_chunks])\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer briefly:\"\n",
    "        answer = llm.invoke(prompt).content.strip()\n",
    "       \n",
    "        state[\"answer\"] = answer\n",
    "    except Exception as e:\n",
    "        state[\"answer\"] = f\"Error using retriever: {e}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 7Ô∏è‚É£  INTENT / GREET / IGNORE NODES\n",
    "# ==========================================================\n",
    "def intent_node(state: dict):\n",
    "    question = state[\"question\"]\n",
    "    prompt = f\"\"\"\n",
    "    Classify this question as one of:\n",
    "    1. numeric ‚Üí structured, count, total, avg\n",
    "    2. semantic ‚Üí descriptive (clients, items, color, etc.)\n",
    "    3. hybrid ‚Üí both numeric + semantic\n",
    "    4. greet ‚Üí greetings\n",
    "    5. ignore ‚Üí unrelated\n",
    "\n",
    "    Question: {question}\n",
    "    Return only one word: numeric, semantic, hybrid, greet, or ignore.\n",
    "    \"\"\"\n",
    "    intent = llm.invoke(prompt).content.strip().lower()\n",
    "    print(f\"üéØ Detected intent ‚Üí {intent}\")\n",
    "    state[\"intent\"] = intent\n",
    "    return state\n",
    "\n",
    "\n",
    "def greet_node(state: dict):\n",
    "    state[\"answer\"] = \"Hello üëã! How can I assist you with the order data today?\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def ignore_node(state: dict):\n",
    "    state[\"answer\"] = \"I'm designed to answer questions about the order dataset. Please ask something related.\"\n",
    "    return state\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 8Ô∏è‚É£  HYBRID NODE ‚Äî COMBINES NUMERIC + SEMANTIC\n",
    "# ==========================================================\n",
    "def hybrid_node(state: dict):\n",
    "    try:\n",
    "        question = state[\"question\"]\n",
    "        print(f\"\\nüîÄ [Hybrid Node] Received question ‚Üí {question}\")\n",
    "\n",
    "        split_prompt = f\"\"\"\n",
    "        Split the query into numeric and semantic parts in JSON.\n",
    "        Example:\n",
    "        {{\n",
    "          \"numeric\": \"subquestion for numeric logic\",\n",
    "          \"semantic\": \"subquestion for semantic logic\"\n",
    "        }}\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        split_result = llm.invoke(split_prompt).content.strip()\n",
    "\n",
    "        import json\n",
    "        split_result = split_result.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        parsed = json.loads(split_result)\n",
    "        numeric_part = parsed.get(\"numeric\", \"\").strip()\n",
    "        semantic_part = parsed.get(\"semantic\", \"\").strip()\n",
    "\n",
    "        print(f\"üìä Numeric part ‚Üí {numeric_part}\")\n",
    "        print(f\"üí¨ Semantic part ‚Üí {semantic_part}\")\n",
    "\n",
    "        numeric_answer, semantic_answer = \"\", \"\"\n",
    "\n",
    "        if numeric_part:\n",
    "            print(f\"üì§ Sending numeric part to DuckDB: {numeric_part}\")\n",
    "            temp_state = {\"question\": numeric_part, \"intent\": \"\", \"context\": [], \"answer\": \"\"}\n",
    "            numeric_state = duckdb_node(temp_state)\n",
    "            numeric_answer = numeric_state.get(\"answer\", \"\")\n",
    "            print(f\"üì• Received numeric answer ‚Üí {numeric_answer}\")\n",
    "\n",
    "        if semantic_part:\n",
    "            print(f\"üì§ Sending semantic part to Retriever: {semantic_part}\")\n",
    "            temp_state = {\"question\": semantic_part, \"intent\": \"\", \"context\": [], \"answer\": \"\"}\n",
    "            semantic_state = retriever_node(temp_state)\n",
    "            semantic_answer = semantic_state.get(\"answer\", \"\")\n",
    "            print(f\"üì• Received semantic answer ‚Üí {semantic_answer}\")\n",
    "\n",
    "        combine_prompt = f\"\"\"\n",
    "        Combine the following insights into a single, clear answer:\n",
    "        Numeric insight: {numeric_answer}\n",
    "        Semantic insight: {semantic_answer}\n",
    "        \"\"\"\n",
    "        final_answer = llm.invoke(combine_prompt).content.strip()\n",
    "        print(f\"üß© Final combined answer ‚Üí {final_answer}\")\n",
    "        state[\"answer\"] = final_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        state[\"answer\"] = f\"Error in hybrid node: {e}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 9Ô∏è‚É£  BUILD STATE GRAPH\n",
    "# ==========================================================\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    intent: str\n",
    "    context: List[str]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"intent\", intent_node)\n",
    "graph.add_node(\"greet\", greet_node)\n",
    "graph.add_node(\"ignore\", ignore_node)\n",
    "graph.add_node(\"duckdb\", duckdb_node)\n",
    "graph.add_node(\"retriever\", retriever_node)\n",
    "graph.add_node(\"hybrid\", hybrid_node)\n",
    "\n",
    "graph.set_entry_point(\"intent\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"intent\",\n",
    "    lambda state: state[\"intent\"],\n",
    "    {\n",
    "        \"greet\": \"greet\",\n",
    "        \"ignore\": \"ignore\",\n",
    "        \"numeric\": \"duckdb\",\n",
    "        \"semantic\": \"retriever\",\n",
    "        \"hybrid\": \"hybrid\",\n",
    "    },\n",
    ")\n",
    "\n",
    "graph.add_edge(\"greet\", END)\n",
    "graph.add_edge(\"ignore\", END)\n",
    "graph.add_edge(\"duckdb\", END)\n",
    "graph.add_edge(\"retriever\", END)\n",
    "graph.add_edge(\"hybrid\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# ==========================================================\n",
    "# üîü  MAIN LOOP\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nüöÄ Smart Query Assistant ready! Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        print(f\"You :\",user_input)\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Assistant: Goodbye üëã\")\n",
    "            break\n",
    "\n",
    "        result = app.invoke({\"question\": user_input})\n",
    "        print(f\"Assistant: {result['answer']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
