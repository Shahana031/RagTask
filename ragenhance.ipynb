{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10301979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict , List\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.retrievers import BM25Retriever,EnsembleRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "import warnings\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9bdf9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    question : str\n",
    "    intent : str\n",
    "    context : List[str]\n",
    "    answer:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a272da8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"keys-to-trading-gold-ca.pdf\")\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3032aa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created:Â 44\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 500,chunk_overlap = 50)\n",
    "texts = splitter.split_documents(documents)\n",
    "print(f\"Total chunks created:Â {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f0f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "llm  = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "persist_directory = r\"D:\\RAG Task\"\n",
    "collection_name =\"article_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e56bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caadfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=texts,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2290236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\":4})\n",
    "keyword_retriever=BM25Retriever.from_documents(texts)\n",
    "\n",
    "hybrid_retriever=EnsembleRetriever(\n",
    "    retrievers=[vector_retriever,keyword_retriever],\n",
    "    weights=[0.6,0.4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbd38261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_node(state : GraphState):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    classification_prompt = f\"\"\"\n",
    "    \n",
    "    Classify the following question into one of these categories:\n",
    "    1. greeting\n",
    "    2. relevant_to_document\n",
    "    3. irrelevant\n",
    "    \n",
    "    Question : \"{question}\"\n",
    "    \n",
    "    Reply only with the category name.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = llm.invoke(classification_prompt).content.strip().lower()\n",
    "    \n",
    "    return {\"intent\" : result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aaf2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet_node(state:GraphState):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a friendly AI assistant.\n",
    "    The user greeted you with: \"{question}\".\n",
    "    Respond politely and naturally, as if you are having a short chat before helping with their document.\n",
    "    Example responses: \"Hey there! Howâ€™s it going?\" or \"Hello! How can I assist you today?\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b7e7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_node(state:GraphState):\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant specialized in answering questions related only to a provided document.\n",
    "    The user asked: \"{question}\".\n",
    "    Politely tell the user that you can only answer questions related to the document content.\n",
    "    Example responses:\n",
    "    - \"I'm sorry, I can only help with questions about the uploaded document.\"\n",
    "    - \"That seems unrelated to the document. Could you please ask something based on it?\"\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\":response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7101142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_node(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    \n",
    "    hybrid_docs = hybrid_retriever.invoke(question)\n",
    "\n",
    "    \n",
    "    pairs = [(question, doc.page_content) for doc in hybrid_docs]\n",
    "\n",
    "    \n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    \n",
    "    ranked_docs = [doc for _, doc in sorted(zip(scores, hybrid_docs), key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "    \n",
    "    top_docs = ranked_docs[:3]\n",
    "\n",
    "  \n",
    "    context = [doc.page_content for doc in top_docs]\n",
    "\n",
    "    return {\"context\": context}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "410a56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_node(state: GraphState):\n",
    "    question = state.get(\"question\", \"\")\n",
    "    context = state.get(\"context\", \"\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant. Use the context below to answer the user's question.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    If the answer is not found in the context, say \"I'm not sure based on the available information.\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f19e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_from_intent(state:GraphState):\n",
    "    intent = state[\"intent\"].lower()\n",
    "    \n",
    "    if \"greeting\" in intent:\n",
    "        return \"greet\"\n",
    "    elif \"relevant\" in intent:\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        return \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d574fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(GraphState)\n",
    "\n",
    "\n",
    "graph.add_node(\"intent\", intent_node)\n",
    "graph.add_node(\"greet\", greet_node)\n",
    "graph.add_node(\"ignore\", ignore_node)\n",
    "graph.add_node(\"retrieve\", retriever_node)\n",
    "graph.add_node(\"answer\", answer_node)\n",
    "\n",
    "graph.add_edge(START, \"intent\")\n",
    "graph.add_conditional_edges(\"intent\", route_from_intent, [\"greet\", \"retrieve\", \"ignore\"])\n",
    "graph.add_edge(\"retrieve\", \"answer\")\n",
    "\n",
    "\n",
    "graph.add_edge(\"greet\", END)\n",
    "graph.add_edge(\"ignore\", END)\n",
    "graph.add_edge(\"answer\", END)\n",
    "\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "529d03b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: RAG Assistant is ready! Type 'exit' or 'quit' to stop.\n",
      "\n",
      "You :exit\n",
      "Assistant: Goodbye! ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "print(\"AI: RAG Assistant is ready! Type 'exit' or 'quit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print(f\"You :{user_input}\")\n",
    "    \n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Assistant: Goodbye! ðŸ‘‹\")\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        response =response = app.invoke({\"question\": user_input})\n",
    "        print(\"Assistant:\", response.get(\"answer\", \"No response generated.\"))\n",
    "    except Exception as e:\n",
    "        print(\"âš  Error:\", str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
